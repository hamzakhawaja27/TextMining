{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hamza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from random import shuffle\n",
    "import collections\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/students/u6019739/DocumentAnalysis/Assignment3/starter_code_data/sentences_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-778d50c02768>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdevName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/students/u6019739/DocumentAnalysis/Assignment3/starter_code_data/sentences_dev.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdevlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/students/u6019739/DocumentAnalysis/Assignment3/starter_code_data/labels_dev.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_ngrams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfileRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfileRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtest_ngrams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfileRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-778d50c02768>\u001b[0m in \u001b[0;36mfileRead\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfileRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/students/u6019739/DocumentAnalysis/Assignment3/starter_code_data/sentences_train.txt'"
     ]
    }
   ],
   "source": [
    "def fileRead(fname):\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    return content\n",
    "fname='/students/u6019739/DocumentAnalysis/Assignment3/starter_code_data/sentences_train.txt'\n",
    "lname='/students/u6019739/DocumentAnalysis/Assignment3/starter_code_data/labels_train.txt'\n",
    "devName='/students/u6019739/DocumentAnalysis/Assignment3/starter_code_data/sentences_dev.txt'\n",
    "devlabel='/students/u6019739/DocumentAnalysis/Assignment3/starter_code_data/labels_dev.txt'\n",
    "train_ngrams=fileRead(fname);\n",
    "train_labels=fileRead(lname);   \n",
    "test_ngrams=fileRead(devName);\n",
    "test_labels=fileRead(devlabel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ngram = namedtuple('Ngram', 'sentence classs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#done\n",
    "def tokenize(ngram):\n",
    "    return word_tokenize(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function takes the ngrams and provides two maps key value, value key. key=unique words value=rank according to freq.\n",
    "#done\n",
    "def build_vocab(train_set):\n",
    "    words = list()\n",
    "    for ngram in train_set:\n",
    "        tokens = tokenize(ngram)\n",
    "        words.extend(tokens)    \n",
    "    count = collections.Counter(words).most_common()\n",
    "    word_to_id = dict()\n",
    "    word_to_id['PAD'] = 0\n",
    "    for word, _ in count:\n",
    "        word_to_id[word] = len(word_to_id)\n",
    "    id_to_word = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "    return word_to_id, id_to_word\n",
    "#build_vocab(train_ngrams)\n",
    "#build_vocab(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#done\n",
    "def map_token_seq_to_word_id_seq(token_seq, word_to_id):\n",
    "    return [map_word_to_id(word_to_id,word) for word in token_seq]\n",
    "\n",
    "def map_word_to_id(word_to_id, word):\n",
    "    if word in word_to_id:\n",
    "        return word_to_id[word]\n",
    "    else:\n",
    "        return word_to_id['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(train_set, word_to_id,label_set,label_to_id):\n",
    "    dataset = list();\n",
    "    i=0;\n",
    "    for ngram in train_set:\n",
    "        tokens = tokenize(ngram)\n",
    "        tokens1= tokenize(label_set[i])\n",
    "        word_id_seq = map_token_seq_to_word_id_seq(tokens, word_to_id) # gives sequence numbers(list) of every token\n",
    "        label= label_to_id[tokens1[0]]\n",
    "        i=i+1;\n",
    "        ngram_inst = Ngram(sentence=word_id_seq, classs=label)\n",
    "        dataset.append(ngram_inst)\n",
    "    return dataset\n",
    "word_to_id, id_to_word = build_vocab(train_ngrams)\n",
    "label_to_id, id_to_label =build_vocab(train_labels)\n",
    "train_set=build_dataset(train_ngrams,word_to_id,train_labels,label_to_id)\n",
    "test_set=build_dataset(test_ngrams,word_to_id,test_labels,label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_to_label_vec(label_id, num_labels):\n",
    "    # initialise a zero vector of the length num_words\n",
    "    label_vec = [0] * num_labels\n",
    "    label_vec[label_id] = 1\n",
    "    return label_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_eval(word_to_id,label_to_id,train_dataset, dev_dataset,test_dataset, num_epochs=3, learning_rate=0.1, embedding_dim=10):\n",
    "    num_words = len(word_to_id)\n",
    "    num_labels = len(label_to_id)\n",
    "    # Placeholders are inputs of the computation graph. \n",
    "    input_ngram = tf.placeholder(tf.int32, shape = [None])\n",
    "    input_sens = tf.placeholder(tf.int32, shape = [None])\n",
    "    correct_label = tf.placeholder(tf.float32, shape=[num_labels]) \n",
    "    # Word embeddings are the only parameters of the model\n",
    "  \n",
    "    # bias is not needed for embeddings\n",
    "    # b = tf.Variable(tf.zeros([num_words]))\n",
    "    with tf.Session() as sess:\n",
    "        embeddings = tf.Variable(tf.random_uniform([num_words, embedding_dim], -1.0, 1.0)) # creates a 2 dim array of random numbers with dim(num of words x 10)\n",
    "        embeddings1 = tf.Variable(tf.random_uniform([embedding_dim,num_labels], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, input_ngram)\n",
    "        tmp_m = tf.reduce_sum(embed, 0)\n",
    "        sum_rep = tf.reshape(tmp_m, [1, embedding_dim])\n",
    "        # Formulate word embedding learning as a word prediction task. Note that, no negative sampling is applied here.\n",
    "        y = tf.nn.softmax(tf.matmul(sum_rep, embeddings1))\n",
    "        c=tf.log(y);\n",
    "        d=-tf.reduce_sum(correct_label * tf.log(y), reduction_indices=[1])\n",
    "#        cross_entropy = tf.reduce_mean(-tf.reduce_sum(correct_label * tf.log(y), reduction_indices=[1]))\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(correct_label * tf.log(tf.clip_by_value(y,1e-10,1.0)), reduction_indices=[1]))\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(correct_label, 0))\n",
    "        accuracy = tf.cast(correct_prediction, tf.float32)\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        # Build SGD optimizer\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffle(train_dataset)\n",
    "            for ngram_inst in train_dataset:\n",
    "                # Run one step of SGD to update word embeddings.\n",
    "                train_step.run(feed_dict={input_ngram: ngram_inst.sentence, correct_label: convert_to_label_vec(ngram_inst.classs, num_labels)})\n",
    "      \n",
    "            print('Epoch %d : %s .' % (epoch,compute_accuracy(num_labels, accuracy,input_ngram, correct_label, dev_dataset)))\n",
    "        print('Accuracy on test set %d : %s .' %(epoch,compute_accuracy(num_labels, accuracy,input_ngram, correct_label, test_dataset)))\n",
    "        # input_sens is the placeholder of an input sentence.\n",
    "#        test_results = predict(prediction, input_sens, test_dataset)\n",
    "    return embeddings,embeddings1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(num_labels, accuracy,input_ngram, correct_label, eval_dataset):\n",
    "    num_correct = 0\n",
    "    for ngram_inst in eval_dataset:\n",
    "        num_correct += accuracy.eval(feed_dict={input_ngram: ngram_inst.sentence, correct_label: convert_to_label_vec(ngram_inst.classs, num_labels)})\n",
    "    print('# of correct sentences are %s ' % num_correct)\n",
    "    return num_correct / len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings1,embeddings2=train_eval(word_to_id,label_to_id,train_set, train_set,test_set,num_epochs=3, learning_rate=0.1, embedding_dim=10)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
